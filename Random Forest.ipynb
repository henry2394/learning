{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "       Hiểu cơ bản là 1 model sử dụng nhiều mô hình tree decision. Mỗi mô hình tree decision sẽ đưa ra dự đoán. Tổng hợp các dự đoán này lại và lựa chọn kết quả có số lần vote cao nhất là kết quả cuối cùng. \n",
    "       \n",
    "# Các bước trong Random Forest\n",
    "1, Tạo các Bootstrapped Dataset\n",
    "\n",
    "- Chọn ngẫu nhiên N quan sát trong tập training (có chọn lặp lại)\n",
    "\n",
    "2, Tạo Decision Tree sử dụng Bootstrapped Dataset tuy nhiên chỉ chọn ngẫu nhiên n feature trong tổng số N feature ban đầu cho mỗi tree và sử dụng n feature con này để xây tại mỗi step\n",
    "- n Feature bao nhiêu là đủ thì phải test thay đổi n nhiều lần để check kết quả --> lựa chọn n feature sao cho kết quả tốt nhất. \n",
    "\n",
    "Vd: sử dụng 2 Features hay 3 Features ...\n",
    "\n",
    "3, Các Tree model con này sẽ được chia tới mức cao nhất\n",
    "\n",
    "4, Dự đoán bằng cách lấy trung bình của các kết quả từ mỗi tree model con (regression) hoặc lấy kết quả được lựa chọn nhiều nhất (classification)\n",
    "\n",
    "5, Dự đoán độ chính xác của Random Forest\n",
    "- Khi ta xây dựng các Bootstrapped Dataset sẽ có nhiều sample không được chọn (Out-Of-Bag Dataset) \n",
    "- Sử dụng RF để predict các sample này và ghi nhận kết quả\n",
    "\n",
    "# Lưu ý khi sử dụng mô hình\n",
    "1 ,Cần biết được các tính năng có mức ảnh hưởng mạnh tới kết quả dự đoán. Rốt cuộc, nếu chúng ta bỏ các feature yếu vào thì mô hình sẽ kém\n",
    "\n",
    "2, Các tree models cần không tương quan với nhau (hoặc ít nhất là có mối tương quan thấp với nhau) như vậy mỗi tree model sẽ phản biện lỗi của nhau tốt hơn (Miễn là chúng không cùng sai về 1 hướng)\n",
    "\n",
    "###### Cách Random Forest đảm bảo mỗi tree model không quá tương quan với nhau. \n",
    "    1: Các Tree model rất nhạy cảm với dữ liệu mà chúng được huấn luyện - những thay đổi nhỏ đối với tập huấn luyện có thể dẫn đến cấu trúc cây khác nhau đáng kể. Tận dụng lợi thế này, từng tree model sẽ lấy mẫu ngẫu nhiên từ tập dữ liệu, dẫn đến các tree model sẽ khác nhau. Ở đây thì sẽ không chia nhỏ data ra mỗi cây train 1 đoạn mà sẽ lấy đúng bằng số lượng gốc.\n",
    "    2: Chọn Feature ngẫu nhiên - Điều này buộc các cây trong mô hình có nhiều biến động hơn và cuối cùng dẫn đến sự tương quan giữa các cây thấp hơn và đa dạng hóa hơn.\n",
    "\n",
    "\n",
    "# Lý do lựa chọn Random Forest\n",
    "###### 1, Tính linh hoạt\n",
    "- Có thể dùng cho cả regression hoặc classification task. Nó có thể hoạt động được với các feature là Category hoặc Numeric\n",
    "- Dữ liệu không cần xử lý nhiều hoặc rescale hoặc transform \n",
    "\n",
    "###### 2, Có thể chạy song song trên nhiều máy\n",
    "- Sử dụng N-job = -1 (tìm hiểu sau)\n",
    "\n",
    "###### 3, Hoạt động tốt với dataset có nhiều dimension (vì chúng chỉ hoạt động với 1 tập feature con được chọn ngẫu nhiên)\n",
    "\n",
    "###### 4, Xử lý tốt các outliers\n",
    "\n",
    "###### 5, Thuật toán Random Forest rất ổn định. \n",
    "Khi một điểm dữ liệu mới được đưa vào tập dữ liệu, thuật toán tổng thể không bị ảnh hưởng nhiều vì dữ liệu mới có thể tác động đến một cây, nhưng rất khó để nó tác động đến tất cả các cây.\n",
    "# Điểm yếu của Random Forest\n",
    "\n",
    "###### 1, Hoạt động không tốt với Regression\n",
    "Do kết quả dự báo đối với Regression là giá trị trung bình --> Kq predict chỉ nằm trong khoảng mà các Tree Models đã dự đoán\n",
    "\n",
    "###### 2, Tốn tài nguyên do phải chạy và tính toán nhiều tree models con\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv(\"04+-+decisiontreeAdultIncome.csv\")\n",
    "data.head()\n",
    "\n",
    "#Create dummy\n",
    "data_prep = pd.get_dummies(data, drop_first=True)\n",
    "data_prep.head()\n",
    "\n",
    "# Define X va Y\n",
    "X = data_prep.drop(['IncomeClass_ >50K'], axis=1)\n",
    "Y = data_prep['IncomeClass_ >50K']\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 1234, stratify = Y) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3881  492]\n",
      " [ 708  856]]\n",
      "0.797877716018191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "y_predict = rf.predict(x_test)\n",
    "# Evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "score = rf.score(x_test, y_test)\n",
    "print(cm)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
